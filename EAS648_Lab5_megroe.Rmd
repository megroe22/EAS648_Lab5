---
title: "EAS648_Lab5_Megroe"
author: "Megan Roe"
date: "2025-11-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Salt Lake City data

Here we will read-in Land Change data and other raster layers that might explain urban development

```{r}  
library(terra)

#first import the land cover/use data 
NLCD_2001 <- rast("NLCD_2001_SL.tif")
NLCD_2004 <- rast("NLCD_2004_SL.tif")
NLCD_2006 <- rast("NLCD_2006_SL.tif")
NLCD_2008 <- rast("NLCD_2008_SL.tif")
NLCD_2011 <- rast("NLCD_2011_SL.tif")
NLCD_2013 <- rast("NLCD_2013_SL.tif")
NLCD_2016 <- rast("NLCD_2016_SL.tif")
# Distance to parks and protected areas in km (Euclidian) for the study area
Park_dist <- rast("Parks_dist_SL.tif")
# Road density for a 1 km neighborhood
Rd_dns1km <- rast("Rd_dns1km_SL.tif")
# Distance to water bodies in km (Euclidean)
WaterDist <- rast("WaterDist_SL.tif")
# elevation
DEM <- rast("DEM_SL.tif")
```

Plotting raster data

```{r}
plot(NLCD_2001)
```

## Building a geospatial database using different methods  
We need a dataset that describes multiple environmental or landscape layers. 

```{r}
allrasters <- c(NLCD_2001, NLCD_2004, NLCD_2006, NLCD_2008, NLCD_2011, NLCD_2013, NLCD_2016, Park_dist, Rd_dns1km, WaterDist,DEM)
```

We can look at all the rasters in the stack of call individual layer
```{r}
#call single raster element
allrasters[[1]]
#to run a function on an individual raster e.g., plot 
plot(allrasters[[1]])
``` 


We can convert the stacked raster into a dataframe. This process essentially extracts the values from each raster layer at every cell location on the map. This transforms the stacked rasters into a tabular format suitable for analysis and visualization. 

```{r}
library(tidyverse)
allrastersSL <- as.data.frame(allrasters, xy=TRUE)
## Here we are filtering out the no data values (stored as 128)
allrastersSL <- allrastersSL %>%
  filter (NLCD_2001_SL != 128)
head(allrastersSL)
```

## Sampling
If we want to use random sample, which is often needed in very large datasets, we can do that.

We  will use the random sample function to grab a spatial random sample
```{r}
library(leaflet)
sampleSLrnd <- spatSample(allrasters, size=100, "random", cells=TRUE, xy=TRUE)
head(sampleSLrnd)
plot(sampleSLrnd$x, sampleSLrnd$y)

```

We can also sample based on a regular grid using 
```{r}
sampleSLreg <- spatSample(allrasters, size=100,  "regular", cells=TRUE, xy=TRUE)
head(sampleSLreg)
plot(sampleSLreg$x, sampleSLreg$y)
```

### Assessing our dataset
We can use the simple `summary()` and `hist()` functions to evaluate data 

```{r}
summary(sampleSLreg)

```

#### Spatial autocorrelation
We can also assess spatial dependency. First we need to transform the data into spatial weights matrix that indicates the  geographic relationship between observations. With our matrix, we can assess the specific spatial autocorrelation of our independent variables. We will simply use Moran's I, which is the most widely use. 
```{r}
# flatten the spatial data to a dataframe that has lat and long
flat_data <- as.data.frame(sampleSLrnd)
# calculate distances between all the points,    
dist_matrix <- as.matrix(dist(cbind(flat_data$x, flat_data$y)))
# and generate a matrix of inverse distance weights.
dist_matix.inv <- 1/dist_matrix
diag(dist_matix.inv) <- 0
```

Now we can measure spatial autocorrelation of the variables
```{r}
library(ape) 
Moran.I(sampleSLrnd$Rd_dns1km_SL, dist_matix.inv, na.rm = TRUE)
```


### statistical analysis
Now that we have some different samples of the data, let's do some analysis. We will be looking at the location characteristics of new development in Salt Lake. First we will identify the pixels that have change to urban classes between 2001 and 2016. 

```{r}
allrastersSL <- allrastersSL %>%
    mutate(urbanChg = (NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) &  (NLCD_2016_SL == 21 | NLCD_2016_SL == 22  | NLCD_2016_SL == 23 | NLCD_2016_SL == 24)) 
    
```

We can map the results
```{r}
ggplot(allrastersSL, aes(y=y, x=x, color=urbanChg)) +
   geom_point(size=2, shape=15) +
   theme() 

```


How much change did Salt Lake experience from 2001 to 2016:

```{r}
## calculate total new urban impervious for 2016
newUrban <- (sum(as.numeric(allrastersSL$NLCD_2016_SL == 21 | allrastersSL$NLCD_2016_SL == 22 |allrastersSL$NLCD_2016_SL == 23 | allrastersSL$NLCD_2016_SL == 24))) - (sum(as.numeric(allrastersSL$NLCD_2001_SL == 21| allrastersSL$NLCD_2001_SL == 22| allrastersSL$NLCD_2001_SL == 23| allrastersSL$NLCD_2001_SL == 24)))
## calculate total urban impervious for 2001
urban2001 <- (sum(as.numeric(allrastersSL$NLCD_2001_SL == 21| allrastersSL$NLCD_2001_SL == 22| allrastersSL$NLCD_2001_SL == 23| allrastersSL$NLCD_2001_SL == 24)))
## percentage increase in urban impervious
newUrban/urban2001* 100
```

We can compare means and plot the variance between newly developed urban areas and the different variables. Let's look at the influence of the distance to protected areas.  

```{r}
allrastersSL %>%
  filter(NLCD_2001_SL != 21| NLCD_2001_SL != 22| NLCD_2001_SL != 23| NLCD_2001_SL != 24) %>%
ggplot(aes(x=urbanChg, y=Parks_dist_SL)) + 
  geom_boxplot()
```
Let's look at the mean and variance of the variables that might shape where there is new development. We have to reshape the data to achieve this.  

```{r}
library(plyr)
library(reshape2)
library(plotly)

SL <- allrastersSL %>%
  filter(NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) 
SL <- SL[10:14]
SLmelt<-melt(SL)
p <- ggplot(SLmelt, aes(x=urbanChg, y=value,fill=variable))+
    geom_boxplot()+
    facet_grid(.~variable)+
    labs(x="X (binned)")+
    theme(axis.text.x=element_text(angle=-90, vjust=0.4,hjust=1))
p
```

### General linear models (Part 1 of Lab)

For our analysis we will take a random sample of our entire dataset (this demonstrate an alternative random sample of a dataframe).  
```{r}
###Grab all the developed cells (presence)
newUrban <- SL %>%
  filter(urbanChg == TRUE)

###Grab all the nondeveloped and not previously urban cells (absence)
nonUrban <- SL %>%
  filter(urbanChg == FALSE)

###Get a random sample of the absence data  
### that is twice as large as the presence data
index <- sample(1:nrow(nonUrban), (round(nrow(newUrban)* 2)))
SLsampleUrban <- nonUrban[index, ]

### combine the orginal presence and absence data
SLsample <- cbind(SLsampleUrban, newUrban)

###Consider a train and testing sample by futher subsampling the data
index <- sample(1:nrow(SL), (round(nrow(SL)* 0.01)))
SLsample <- SL[index, ]

###Consider making a training and testing dataset
###This can reduce the computational burden 
### It also is a robust goodness of fit method
#SLsample <- SLsample %>% dplyr::mutate(id = row_number())
#Create training set
#train <- SLsample %>% sample_frac(.70)
#Create test set
#test  <- anti_join(SLsample, train, by = 'id')
```

Now that we have a sample, we can fit a model.
```{r}
fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL,data=SLsample,family=binomial())
summary(fit)

```


## **Summary and Interpretation of Model Fit**
From the model above, we see that all variables have three stars of significance except for WaterDist_SL, which has a P-value much larger than the others (but still significant) pr(>|z|)= 0.00136. This variable only has two stars. This model has an AIC of 4170. We can try to run the model again without the water variable and reevaluate the AIC value to see if that model is a better fit. 

We can fit the model without the water distance variable. 
```{r}
fit_withoutwater <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + DEM_SL,data=SLsample,family=binomial())
summary(fit_withoutwater)

```
In this model, we see that the AIC value has increased slightly from 4170 to 1478.2, indicating that the other model is actually the better fit. This means we should add this variable back into the model. So, our **final model fit is presented below**. The model indicates that all of the variables play a significant role in predicting Urban Change. By looking at the estimates (coefficients), we also can see that all variables have a positive association with urban change except for DEM_SL. When we look at the AUC (Area Under the Curve) we get a value of 0.9532678, which is pretty close to 1, meaning pretty good! If we had more variables, we could test those to see if those make a better fit for the urban change model. I think adding a variable that examined affordability factors or possibly distance to major shopping centers (malls, large grocery stores, etc) would also be a predictive variable for urban development. 


Final model fit:
```{r}
fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + DEM_SL + WaterDist_SL,data=SLsample,family=binomial())
summary(fit)

```
### Goodness of fit 

```{r}
library(ROCR)

pred <- prediction(predict(fit), SLsample$urbanChg)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)

auc_ROCR <- performance(pred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR
```



### Predicting likely locations of development
Here we use our predictors to estimate for each pixel the probability of development given our model
```{r}
predicted <- predict(allrasters, fit)
```

Now we can visualize this prediction

```{r}
plot(predicted)
```


## Cluster analysis (Part 2 of Lab)

```{r}
library(fastcluster)
library(graphics)
library(ggplot2)
library(ggdendro)
```

This is a dataset of different amenity characteristic for the entire US. Derek developed with dataset using a 10 km by 10 km grid using different sources of data (e.g. NLCD, [SEDAC], social media, cultural monuments). 

```{r, echo=FALSE}
library(sf)
amenData<- st_read("AmenDataAll.shp")
head(amenData)
```

###Overview of the data


```{r}
ggplot() + 
  geom_sf(data = amenData, mapping = aes(fill = ser00publi), color = NA) + 
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  theme(legend.position = "none") +
  ggtitle("Percentage Public Lands")

```

As we will be doing several operation that cannot use spatial data in the sf format, we will need to save the spatial attributes of the data. This will allow us to apply these spatial attribute after our analysis for mapping. 
```{r}
st_geometry(amenData)
geomData = st_sfc(amenData$geometry)
```
### Tranforming some data to make it meaningful and interpretable

#### Geographic transformations
In geographic relationships, we might assume that the nearer we are to something the more important it is (i.e. there is not a spatial linear relationship with a location attribute). In order to represent this, we can log transform distance variable that we think has this nonlinear relationship. 

```{r}
amenData$HotelEmpDist_log <- log(amenData$HotelDistE + 0.00000001)
amenData$HistMonDist_log <- log(amenData$HistMon_Di + 0.00000001)
amenData$HighEdEmpDist_log <- log(amenData$HigherEdDi + 0.00000001)
amenData$GolfEmpDist_log <- log(amenData$GolfDistEm + 0.00000001)

```

Here we are combining variables to test different hypotheses. 

```{r}
amenData$HousingChg <- amenData$Urb2011 - amenData$Urb2001
```

#### reduce to only the data that we need
In cluster analysis we must provide a data frame or matrix of only the data that will be included in the model. Here, I chose to include the variables that I need to examine percentage of public lands. I chose the varaibles that I thought would have a fairly strong tie to how much public land there is. 
```{r}
amenDataDF<-amenData[,c("HousingChg", "Frst2011", 
                        "WaterPct", "ser00publi", "DEM_max","DEM_range", 
                        "HikeLength","Wd_wetland", "Emg_wetld",
                        "HotelEmpDist_log","HistMonDist_log",
                        "HighEdEmpDist_log", "GolfEmpDist_log")]
###if you want to add variable look at the names using: names(amenData)
```

We must also omit any `NA`s 

```{r}
## make sure there are no missing data
amenDataDF <- na.omit(amenDataDF)
## we need to make it into a normal dataframe to 
## do the analysis
amenDataDF <- as.data.frame(amenDataDF)[1:12]
## calculate z-scores for the dataset
db_scaled <- scale(amenDataDF)
```

### K-means clustering

Typically in kmeans you determine how many clusters to specify beforehand. This can be done by calculating the within groups sum of squares by number of clusters. Basically, we look for a bend in the plot similar to a scree test in factor analysis

```{r}
# Determine number of clusters
wss <- (nrow(db_scaled)-1)*sum(apply(db_scaled,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(db_scaled,
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

Seems like 5

```{r}
# K-Means Cluster Analysis
fit <- kmeans(db_scaled, 5) # 5 cluster solution
```

We can explore the result further by looking at the mean of each variable by the clusters. 
```{r}
# get cluster means
aggregate(db_scaled,by=list(fit$cluster),FUN=mean)
```

 Adding the cluster membership variable back into your original dataset.

```{r}
# append cluster assignment
amenData <- data.frame(amenData, fit$cluster)
```

In order to plot this data we need to steal the geometry from the original data and apply it to this dataset. We did this earlier. 

```{r}
st_geometry(amenData) <- geomData
```

Now we can plot the kmeans clusters. After visualizing the data, you might want to redo the analysis with other numbers of clusters.

```{r}
ggplot() + 
  geom_sf(data = amenData, mapping = aes(fill = as.factor(fit.cluster)), color = NA) + 
  theme(legend.position = "none") +
  ggtitle("Clusters based on Kmeans")
```

## **Summary and Interpretation of Cluster Model**

In this exploratory exercise, I wanted to focus on factors that may be associated with Percent Public Land, so i chose to focus on variables surrounding natural features (water), landscape features (forest coverage), and accessibility (hotel and other recreational activities like golf and hiking trails). 

When we look at our results from the aggregate function, we see that group 1 has some high public land z-value (1.46).We see this in the western part of the country that is more mountainous. This makes sense logically too. We see higher elevations here and more hiking trails/parks.Group 2 is somewhat similar. IN this group we see high hiking tail length z-values (2.26). Group 3 has a very low public land value and low elevation. This seems more like traditional suburban and/or agricultural flat lands. Group 4 has negative values for elevation, elevation range, forest, public land, wetlands, and very negative amenity distances. This seems to overlap with areas that are pretty urbanized/developed, which would likely not have a ton of public lands. Group 5 is super interesting as the water value is *very* high (4.449), indicating that these are coastal, wooded, or wetland regions (3.53 and 3.36). This group has a slight positive value for public land (0.065), and this makes sense as many areas in Florida have public lands dedicated to wetlands or coastal beaches. Overall, we can see a clear tie between percent public land and the mountainous western united states. 